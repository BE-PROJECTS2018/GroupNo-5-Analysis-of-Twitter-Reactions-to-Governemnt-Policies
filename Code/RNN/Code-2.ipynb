{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import preprocessor as p\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('DigitalIndia-labelling-labelled.csv', encoding = 'utf-8')\n",
    "data = data.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis =1)\n",
    "data = data.reset_index(drop = True)\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('DI-labelling.csv', encoding = 'utf-8')\n",
    "test_data = test_data.drop(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 8'], axis =1)\n",
    "test_data = test_data.reset_index(drop = True)\n",
    "print(test_data.head())\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_data = pd.read_csv('FinalPreprocessed-DigitalIndia.csv', encoding = 'utf-8')\n",
    "unlabelled_data = unlabelled_data.drop(['Unnamed: 0'], axis =1)\n",
    "print(unlabelled_data.head())\n",
    "print(unlabelled_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "i = 0\n",
    "while i < data.shape[0]:\n",
    "    pos = data['Positive'][i]\n",
    "    neg = data['Negative'][i]\n",
    "    temp_list = [pos , neg]\n",
    "    labels.append(temp_list)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "i = 0\n",
    "while i < test_data.shape[0]:\n",
    "    pos = test_data['Positive'][i]\n",
    "    neg = test_data['Negative'][i]\n",
    "    temp_list = [pos , neg]\n",
    "    test_labels.append(temp_list)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the Stanford GloVe model\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "glove_model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sequence_length = 55\n",
    "num_dimensions = 100\n",
    "size = data.shape[0]\n",
    "vector_output = np.zeros((size ,max_sequence_length, num_dimensions), dtype='float32')\n",
    "i = 0    \n",
    "while i < size:\n",
    "    tweet_text = data['text'][i]\n",
    "    cleanedLine = cleanSentences(tweet_text)\n",
    "    split = cleanedLine.split()\n",
    "    indexCounter = 0\n",
    "    for word in split:\n",
    "        try:\n",
    "            vector_output[i][indexCounter] = glove_model[word]\n",
    "        except KeyError:\n",
    "            print(split[indexCounter])\n",
    "        \n",
    "        indexCounter = indexCounter + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sequence_length = 55\n",
    "num_dimensions = 100\n",
    "size = unlabelled_data.shape[0]\n",
    "vector_output_unlabelled = np.zeros((size ,max_sequence_length, num_dimensions), dtype='float32')\n",
    "i = 0    \n",
    "while i < size:\n",
    "    tweet_text =unlabelled_data['text'][i]\n",
    "    cleanedLine = cleanSentences(tweet_text)\n",
    "    split = cleanedLine.split()\n",
    "    indexCounter = 0\n",
    "    for word in split:\n",
    "        try:\n",
    "            vector_output_unlabelled[i][indexCounter] = glove_model[word]\n",
    "        except KeyError:\n",
    "            print(split[indexCounter])\n",
    "        \n",
    "        indexCounter = indexCounter + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sequence_length = 55\n",
    "num_dimensions = 100\n",
    "size = test_data.shape[0]\n",
    "vector_output_test = np.zeros((size ,max_sequence_length, num_dimensions), dtype='float32')\n",
    "i = 0    \n",
    "while i < size:\n",
    "    tweet_text = test_data['text'][i]\n",
    "    cleanedLine = cleanSentences(tweet_text)\n",
    "    split = cleanedLine.split()\n",
    "    indexCounter = 0\n",
    "    for word in split:\n",
    "        try:\n",
    "            vector_output_test[i][indexCounter] = glove_model[word]\n",
    "        except KeyError:\n",
    "            print(split[indexCounter])\n",
    "        \n",
    "        indexCounter = indexCounter + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "num_inputs = 55\n",
    "num_dimensions = 100\n",
    "num_outputs = 2\n",
    "num_neurons = 30\n",
    "training_iterations = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#PLACEHOLDERS\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None , num_inputs, num_dimensions], name = 'input_placeholder' )\n",
    "y = tf.placeholder(dtype = tf.int32, shape = [None ,num_outputs], name = 'labels_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RNN cell\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units = num_neurons)\n",
    "lstm_cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.85)\n",
    "output, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.transpose(output, [1, 0, 2])\n",
    "last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "weight = tf.Variable(tf.truncated_normal([num_neurons, num_outputs]), name = 'weight')\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[num_outputs]), name ='bias')\n",
    "prediction =tf.add(tf.matmul(last, weight), bias, name= 'prediction')\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "answer = tf.nn.softmax(prediction)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "train = optimizer.minimize(loss, name ='final_operation')\n",
    "#Saving a model\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(\"./outputModel\", sess.graph)\n",
    "    \n",
    "    for iteration in range(500):\n",
    "        sess.run(train, feed_dict = {X: vector_output, y: labels})\n",
    "        \n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            error = loss.eval(feed_dict = {X:vector_output , y: labels})\n",
    "            print(iteration + 1, \"\\tError:\", error)\n",
    "                \n",
    "    saver.save(sess, \"./rnn_model\", global_step=iteration)\n",
    "    writer.close()\n",
    "    \n",
    "    #testing data\n",
    "    print(\"Testset Accuracy\")\n",
    "    error = loss.eval(feed_dict = {X: vector_output_test , y: test_labels})\n",
    "    print(\"\\tError:\", error)\n",
    "    predictions_testset = answer.eval(feed_dict = {X: vector_output_test})\n",
    "    \n",
    "    #predicting unlabelled data\n",
    "    unlabelled_answer = answer.eval(feed_dict={X:unlabelled_vector_output}, session=sess)\n",
    "    print(unlabelled_answer.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_testlabel = []\n",
    "neg_testlabel = []\n",
    "i = 0\n",
    "correct_counter = 0\n",
    "wrong_counter = 0\n",
    "while i < test_data.shape[0]:\n",
    "    if predictions_testset[i][0] > predictions_testset[i][1]:\n",
    "        pos_testlabel.append(1)\n",
    "        neg_testlabel.append(0)\n",
    "    else:\n",
    "        pos_testlabel.append(0)\n",
    "        neg_testlabel.append(1)\n",
    "    i = i+ 1\n",
    "#print(len(pos_testlabel))\n",
    "#print(testBatchLabels)\n",
    "i = 0\n",
    "while i < test_data.shape[0]:\n",
    "    if pos_testlabel[i] == test_labels[i][0]:\n",
    "        correct_counter = correct_counter + 1\n",
    "    else:\n",
    "        wrong_counter = wrong_counter + 1\n",
    "    i= i + 1\n",
    "print(correct_counter)\n",
    "print(wrong_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer= pd.DataFrame()\n",
    "answer['Positive'] = pos_testlabel\n",
    "answer['Negative'] = neg_testlabel\n",
    "answer.to_csv('answer.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
